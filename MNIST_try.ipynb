{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "2127d30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from skrmt.ensemble.spectral_law import MarchenkoPasturDistribution\n",
    "\n",
    "from typing import Callable, Tuple\n",
    "\n",
    "import torch\n",
    "from diffusers import DDPMPipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "499ecd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def algorithm2_estimate_singular_values(\n",
    "#     score_fn: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n",
    "#     x0: torch.Tensor,                 # shape (1, C, H, W)\n",
    "#     t0: torch.Tensor,                 # shape (1,) or scalar tensor (time / timestep)\n",
    "#     sigma_t0: float | torch.Tensor,   # scalar\n",
    "# ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "#     \"\"\"\n",
    "#     Algorithm 2.\n",
    "\n",
    "#     Require:\n",
    "#       score_fn: fonction qui renvoie s_theta(x, t) (même shape que x)\n",
    "#       x0: échantillon de data-set (1,C,H,W)\n",
    "#       t0: temps (tensor scalaire ou (1,))\n",
    "#       sigma_t0: niveau de bruit (scalaire)\n",
    "\n",
    "#     Returns:\n",
    "#       s: singular values (d,)\n",
    "#       v: left singular vectors (d,d)  (torch.linalg.svd convention: U)\n",
    "#       w: right singular vectors (d,d) (Vh.T)\n",
    "#       S: matrice (d,d)\n",
    "#     \"\"\"\n",
    "\n",
    "#     # 1) Sample x0 ~ p0(x) : fourni en entrée\n",
    "#     # 2) d <- dim(x0)\n",
    "#     d = x0.numel()\n",
    "#     shape = x0.shape\n",
    "\n",
    "#     # 3) S <- empty matrix\n",
    "#     S = torch.empty((d, 0), device=x0.device, dtype=x0.dtype)\n",
    "\n",
    "#     # 4-8) Sample right/left perturbations\n",
    "#     Xp = []\n",
    "#     Xm = []\n",
    "#     for i in range(d):\n",
    "#         # 5) x+ ~ N(x0, sigma^2 I)\n",
    "#         x_plus = x0 + sigma_t0 * torch.randn_like(x0)\n",
    "#         # 6) x- ~ N(x0, sigma^2 I)\n",
    "#         x_minus = x0 + sigma_t0 * torch.randn_like(x0)\n",
    "#         # 7) x- <- 2 x0 - x-\n",
    "#         x_minus = 2 * x0 - x_minus\n",
    "#         Xp.append(x_plus)\n",
    "#         Xm.append(x_minus)\n",
    "\n",
    "#     Xp = torch.cat(Xp, dim=0)  # (d, C, H, W)\n",
    "#     Xm = torch.cat(Xm, dim=0)  # (d, C, H, W)\n",
    "#     print(f\"Xp shape: {Xp.shape}, Xm shape: {Xm.shape}\")\n",
    "\n",
    "#     # 9) orthogonalized perturbations\n",
    "#     # On orthogonalise les deltas autour de x0, puis on reconstruit x~ = x0 + delta~\n",
    "#     x0_flat = x0.reshape(1, d)\n",
    "#     dp = (Xp.reshape(d, d) - x0_flat)  # (d,d) lignes = deltas+\n",
    "#     dm = (Xm.reshape(d, d) - x0_flat)  # (d,d) lignes = deltas-\n",
    "\n",
    "#     # Orthogonalisation séparée pour + et - (le papier dit (x+, x-) -> orthogonalized)\n",
    "#     # Ici on applique QR aux transposées pour rendre les LIGNES orthonormales.\n",
    "#     Qp, _ = torch.linalg.qr(dp.T)  # (d,d)\n",
    "#     Qm, _ = torch.linalg.qr(dm.T)  # (d,d)\n",
    "\n",
    "#     dp_ortho = Qp.T\n",
    "#     dm_ortho = Qm.T\n",
    "\n",
    "#     Xp_tilde = (x0_flat + dp_ortho).reshape(d, *shape[1:])  # (d,C,H,W)\n",
    "#     Xm_tilde = (x0_flat + dm_ortho).reshape(d, *shape[1:])  # (d,C,H,W)\n",
    "#     # Xp_tilde = ((torch.linalg.qr(Xp.reshape(d, d))[0])).reshape(d, *shape[1:])  # (d,C,H,W)\n",
    "#     # Xm_tilde = ((torch.linalg.qr(Xm.reshape(d, d))[0])).reshape(d, *shape[1:])  # (d,C,H,W)\n",
    "#     # Xp_tilde = ((torch.linalg.qr(Xp.reshape(d, d))[0])).reshape(d, *shape[1:])  # (d,C,H,W)\n",
    "#     # Xm_tilde = ((torch.linalg.qr(Xm.reshape(d, d))[0])).reshape(d, *shape[1:])  # (d,C,H,W)\n",
    "\n",
    "#     # 10-12) remplir S colonne par colonne\n",
    "#     # (On suit exactement: append (s(x+)-s(x-))/2 as new column)\n",
    "#     for i in range(d):\n",
    "#         s_plus = score_fn(Xp_tilde[i:i+1], t0)   # (1,C,H,W)\n",
    "#         s_minus = score_fn(Xm_tilde[i:i+1], t0)  # (1,C,H,W)\n",
    "#         col = ((s_plus - s_minus) / 2.0).reshape(d, 1)      # (d,1)\n",
    "#         S = torch.cat([S, col], dim=1)  # append column\n",
    "\n",
    "#     # 13) SVD(S)\n",
    "#     # torch.linalg.svd retourne U, S, Vh avec S non vectorisée? -> S est vecteur des SVs\n",
    "#     U, sing_vals, Vh = torch.linalg.svd(S, full_matrices=True)\n",
    "\n",
    "#     # papier: (si), (vi), (wi) <- SVD(S)\n",
    "#     # on renvoie sing_vals, U, V (où V = Vh^T), et S\n",
    "#     V = Vh.transpose(-2, -1)\n",
    "\n",
    "#     return sing_vals, U, V, S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "2592030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def algorithm2_estimate_singular_values(\n",
    "    score_fn: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n",
    "    x0: torch.Tensor,                 # shape (1, C, H, W)\n",
    "    t0: torch.Tensor,                 # shape (1,) or scalar tensor (time / timestep)\n",
    "    sigma_t0: float | torch.Tensor,   # scalar\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Algorithm 2.\n",
    "\n",
    "    Require:\n",
    "      score_fn: fonction qui renvoie s_theta(x, t) (même shape que x)\n",
    "      x0: échantillon de data-set (1,C,H,W)\n",
    "      t0: temps (tensor scalaire ou (1,))\n",
    "      sigma_t0: niveau de bruit (scalaire)\n",
    "\n",
    "    Returns:\n",
    "      s: singular values (d,)\n",
    "      v: left singular vectors (d,d)  (torch.linalg.svd convention: U)\n",
    "      w: right singular vectors (d,d) (Vh.T)\n",
    "      S: matrice (d,d)\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Sample x0 ~ p0(x) : fourni en entrée\n",
    "    # 2) d <- dim(x0)\n",
    "    d = x0.numel()\n",
    "    shape = x0.shape\n",
    "\n",
    "    # 3) S <- empty matrix\n",
    "    S = torch.empty((d, 0), device=x0.device, dtype=x0.dtype)\n",
    "\n",
    "    # 4-8) Sample right/left perturbations\n",
    "    Xp = []\n",
    "    Xm = []\n",
    "    for i in range(d):\n",
    "        n = torch.randn_like(x0)\n",
    "        # 5) x+ ~ N(x0, sigma^2 I)\n",
    "        x_plus = x0 + sigma_t0 * n\n",
    "        # 6) x- ~ N(x0, sigma^2 I)\n",
    "        x_minus = x0 + sigma_t0 * n\n",
    "        # 7) x- <- 2 x0 - x-\n",
    "        x_minus = 2 * x0 - x_minus\n",
    "        Xp.append(x_plus)\n",
    "        Xm.append(x_minus)\n",
    "\n",
    "    Xp = torch.cat(Xp, dim=0)  # (d, C, H, W)\n",
    "    Xm = torch.cat(Xm, dim=0)  # (d, C, H, W)\n",
    "    print(f\"Xp shape: {Xp.shape}, Xm shape: {Xm.shape}\")\n",
    "\n",
    "    # 9) orthogonalized perturbations\n",
    "    # On orthogonalise les deltas autour de x0, puis on reconstruit x~ = x0 + delta~\n",
    "    # x0_flat = x0.reshape(1, d)\n",
    "    # dp = (Xp.reshape(d, d) - x0_flat)  # (d,d) lignes = deltas+\n",
    "    # dm = (Xm.reshape(d, d) - x0_flat)  # (d,d) lignes = deltas-\n",
    "\n",
    "    # # Orthogonalisation séparée pour + et - (le papier dit (x+, x-) -> orthogonalized)\n",
    "    # # Ici on applique QR aux transposées pour rendre les LIGNES orthonormales.\n",
    "    # Qp, _ = torch.linalg.qr(dp.T)  # (d,d)\n",
    "    # Qm, _ = torch.linalg.qr(dm.T)  # (d,d)\n",
    "\n",
    "    # dp_ortho = Qp.T\n",
    "    # dm_ortho = Qm.T\n",
    "\n",
    "    # # print(dp_ortho)\n",
    "    # # dp_ortho = dp_ortho * dp.norm(dim=1, keepdim=True)\n",
    "    # # print(f\"Norms dp: {dp.norm(dim=1, keepdim=True)[0]}\")\n",
    "    # # print(dp_ortho)\n",
    "\n",
    "    # # dm_ortho = dm_ortho * dm.norm(dim=1, keepdim=True)\n",
    "\n",
    "    # Xp_tilde = (x0_flat + dp_ortho).reshape(d, *shape[1:])  # (d,C,H,W)\n",
    "    # Xm_tilde = (x0_flat + dm_ortho).reshape(d, *shape[1:])  # (d,C,H,W)\n",
    "\n",
    "    ################################################################################\n",
    "\n",
    "    x0_flat = x0.reshape(-1)              # (d,)\n",
    "    d = x0_flat.numel()\n",
    "\n",
    "    # 1) Sample right perturbations: x_plus ~ N(x0, sigma^2 I)\n",
    "    #    => delta = x_plus - x0 = sigma * z\n",
    "    Zp = torch.randn(d, d, device=device)  # (d,d)\n",
    "    Zm = torch.randn(d, d, device=device)  # (d,d)\n",
    "    # colonne j : une direction brute\n",
    "    # delta_j = sigma * Z[:,j]\n",
    "\n",
    "    # 2) Orthonormalize directions (QR): Q colonnes orthonormales\n",
    "    Qp, _ = torch.linalg.qr(Zp)  # Q: (d,d)\n",
    "    Qm, _ = torch.linalg.qr(Zm)  # Q: (d,d)\n",
    "\n",
    "    # 3) Build paired perturbations, couplées et symétriques\n",
    "    #    Ici chaque déplacement a norme ~1, donc pas = sigma constant\n",
    "    deltasp = sigma_t0 * Qp  # (d,d), colonne j = sigma*q_j\n",
    "    deltasm = sigma_t0 * Qm  # (d,d), colonne j = sigma*q_j\n",
    "\n",
    "    # On renvoie des batchs de taille d\n",
    "    # Chaque élément est une image/vec perturbé\n",
    "    x_plus_tilde  = (x0_flat[:, None] + deltasp).T  # (d,d) -> (d,d)\n",
    "    x_minus_tilde = (x0_flat[:, None] - deltasm).T  # (d,d)\n",
    "\n",
    "    # Remet en forme originale (d, *x0.shape)\n",
    "    x_plus_tilde  = x_plus_tilde.reshape(d, *shape[1:])\n",
    "    x_minus_tilde = x_minus_tilde.reshape(d, *shape[1:])\n",
    "\n",
    "    Xp_tilde = x_plus_tilde\n",
    "    Xm_tilde = x_minus_tilde\n",
    "\n",
    "    # 10-12) remplir S colonne par colonne\n",
    "    # (On suit exactement: append (s(x+)-s(x-))/2 as new column)\n",
    "    for i in range(d):\n",
    "        s_plus = score_fn(Xp_tilde[i:i+1], t0)   # (1,C,H,W)\n",
    "        s_minus = score_fn(Xm_tilde[i:i+1], t0)  # (1,C,H,W)\n",
    "        col = ((s_plus - s_minus) / 2.0).reshape(d, 1)      # (d,1)\n",
    "        S = torch.cat([S, col], dim=1)  # append column\n",
    "\n",
    "    # 13) SVD(S)\n",
    "    # torch.linalg.svd retourne U, S, Vh avec S non vectorisée? -> S est vecteur des SVs\n",
    "    U, sing_vals, Vh = torch.linalg.svd(S, full_matrices=True)\n",
    "\n",
    "    # papier: (si), (vi), (wi) <- SVD(S)\n",
    "    # on renvoie sing_vals, U, V (où V = Vh^T), et S\n",
    "    V = Vh.transpose(-2, -1)\n",
    "\n",
    "    return sing_vals, U, V, S\n",
    "\n",
    "# # test the algorithm\n",
    "# alpha_bar = scheduler.alphas_cumprod[0].to(x0.dtype).view(-1, 1, 1, 1)\n",
    "# sigma_t = torch.sqrt(1.0 - alpha_bar)\n",
    "\n",
    "# res = algorithm2_estimate_singular_values(\n",
    "#     score_fn,\n",
    "#     x0,\n",
    "#     torch.tensor([0], device=device),\n",
    "#     sigma_t0=sigma_t,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "e5f9ddba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def algorithm1_estimate_singular_values(\n",
    "    score_fn: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n",
    "    x0: torch.Tensor,                 # shape (1, C, H, W)\n",
    "    t0: torch.Tensor,                 # shape (1,) or scalar tensor (time / timestep)\n",
    "    sigma_t0: float | torch.Tensor,   # scalar\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Algorithm 2.\n",
    "\n",
    "    Require:\n",
    "      score_fn: fonction qui renvoie s_theta(x, t) (même shape que x)\n",
    "      x0: échantillon de data-set (1,C,H,W)\n",
    "      t0: temps (tensor scalaire ou (1,))\n",
    "      sigma_t0: niveau de bruit (scalaire)\n",
    "\n",
    "    Returns:\n",
    "      s: singular values (d,)\n",
    "      v: left singular vectors (d,d)  (torch.linalg.svd convention: U)\n",
    "      w: right singular vectors (d,d) (Vh.T)\n",
    "      S: matrice (d,d)\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Sample x0 ~ p0(x) : fourni en entrée\n",
    "    # 2) d <- dim(x0)\n",
    "    d = x0.numel()\n",
    "    shape = x0.shape\n",
    "\n",
    "    # 3) S <- empty matrix\n",
    "    S = torch.empty((d, 0), device=x0.device, dtype=x0.dtype)\n",
    "\n",
    "    # 4-8) Sample right/left perturbations\n",
    "    X = []\n",
    "    for i in range(d):\n",
    "        n = torch.randn_like(x0)\n",
    "        x = x0 + sigma_t0 * n\n",
    "        X.append(x)\n",
    "\n",
    "    X = torch.cat(X, dim=0)\n",
    "\n",
    "    Q, _ = torch.linalg.qr(X.reshape(d, d).T)\n",
    "    # bring back Q norm\n",
    "    Q = Q * X.reshape(d, d).norm(dim=0, keepdim=True)\n",
    "    Q = Q.reshape(d, *shape[1:])\n",
    "    print(X.reshape(d, d).norm(dim=0, keepdim=True))\n",
    "\n",
    "    for i in range(d):\n",
    "        s = score_fn(Q[i:i+1], t0).reshape(d, 1)    \n",
    "        S = torch.cat([S, s], dim=1)  # append column\n",
    "\n",
    "    U, sing_vals, Vh = torch.linalg.svd(S, full_matrices=True)\n",
    "\n",
    "    return sing_vals, U, Vh.T, S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "3b6042d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 2/2 [00:00<00:00, 99.52it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UNet2DModel(\n",
       "  (conv_in): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (time_proj): Timesteps()\n",
       "  (time_embedding): TimestepEmbedding(\n",
       "    (linear_1): Linear(in_features=32, out_features=128, bias=True)\n",
       "    (act): SiLU()\n",
       "    (linear_2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       "  (down_blocks): ModuleList(\n",
       "    (0): DownBlock2D(\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 32, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (norm2): GroupNorm(32, 32, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (downsamplers): ModuleList(\n",
       "        (0): Downsample2D(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): AttnDownBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-1): 2 x Attention(\n",
       "          (group_norm): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "          (to_q): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (to_k): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (to_v): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 32, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=128, out_features=64, bias=True)\n",
       "          (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=128, out_features=64, bias=True)\n",
       "          (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (downsamplers): ModuleList(\n",
       "        (0): Downsample2D(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): DownBlock2D(\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (norm2): GroupNorm(32, 32, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 32, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (norm2): GroupNorm(32, 32, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up_blocks): ModuleList(\n",
       "    (0): UpBlock2D(\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (norm2): GroupNorm(32, 32, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 96, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (norm2): GroupNorm(32, 32, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (upsamplers): ModuleList(\n",
       "        (0): Upsample2D(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): AttnUpBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-2): 3 x Attention(\n",
       "          (group_norm): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "          (to_q): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (to_k): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (to_v): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 96, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(96, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=128, out_features=64, bias=True)\n",
       "          (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=128, out_features=64, bias=True)\n",
       "          (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 96, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(96, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=128, out_features=64, bias=True)\n",
       "          (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (upsamplers): ModuleList(\n",
       "        (0): Upsample2D(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): UpBlock2D(\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 96, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (norm2): GroupNorm(32, 32, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1-2): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (norm2): GroupNorm(32, 32, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mid_block): UNetMidBlock2D(\n",
       "    (attentions): ModuleList(\n",
       "      (0): Attention(\n",
       "        (group_norm): GroupNorm(32, 32, eps=1e-05, affine=True)\n",
       "        (to_q): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (to_k): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (to_v): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (resnets): ModuleList(\n",
       "      (0-1): 2 x ResnetBlock2D(\n",
       "        (norm1): GroupNorm(32, 32, eps=1e-05, affine=True)\n",
       "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (time_emb_proj): Linear(in_features=128, out_features=32, bias=True)\n",
       "        (norm2): GroupNorm(32, 32, eps=1e-05, affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (nonlinearity): SiLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv_norm_out): GroupNorm(32, 32, eps=1e-05, affine=True)\n",
       "  (conv_act): SiLU()\n",
       "  (conv_out): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "pipe = DDPMPipeline.from_pretrained(\"1aurent/ddpm-mnist\")\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "unet = pipe.unet\n",
    "scheduler = pipe.scheduler\n",
    "\n",
    "unet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "5eaf0294",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_fn(x, t0):\n",
    "    # x: (B,1,28,28), t0: tensor (1,) ou (B,)\n",
    "    if t0.ndim == 1 and t0.shape[0] == 1:\n",
    "        t = t0.expand(x.shape[0]).to(device)\n",
    "    else:\n",
    "        t = t0.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        eps_pred = unet(x, t).sample  # (B,1,28,28)\n",
    "\n",
    "    # sigma_t = sqrt(1 - alpha_bar_t)\n",
    "    alpha_bar = scheduler.alphas_cumprod[t].to(x.dtype).view(-1, 1, 1, 1)\n",
    "    sigma_t = torch.sqrt(1 - alpha_bar)\n",
    "\n",
    "    # score ≈ - eps / sigma\n",
    "    return -eps_pred / sigma_t\n",
    "    #return -eps_pred / np.sqrt(t + 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "a8fd68d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 2/2 [00:00<00:00, 83.25it/s]\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# MNIST: on met dans [-1, 1] (standard pour diffusion)\n",
    "tfm = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: 2 * x - 1),\n",
    "])\n",
    "\n",
    "mnist = datasets.MNIST(root=\"./data\", train=False, download=True, transform=tfm)\n",
    "x0, y = mnist[101]                      # x0: (1,28,28)\n",
    "x0 = x0.unsqueeze(0).to(device)       # (1,1,28,28)\n",
    "\n",
    "\n",
    "\n",
    "# Modèle diffusion MNIST (pré-entraîné HF)\n",
    "pipe = DDPMPipeline.from_pretrained(\"1aurent/ddpm-mnist\").to(device)\n",
    "unet = pipe.unet.eval()\n",
    "scheduler = pipe.scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "afe52871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_fn(x0, torch.tensor([0], device=device))[0, 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "73bc1a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27.9910, 27.9866, 28.0194, 28.0010, 28.0093, 28.0051, 27.9990, 27.9948,\n",
      "         27.9952, 27.9949, 27.9897, 27.9921, 28.0076, 27.9845, 28.0063, 28.0041,\n",
      "         28.0001, 27.9909, 28.0242, 28.0085, 28.0099, 28.0074, 27.9915, 28.0009,\n",
      "         27.9946, 28.0000, 28.0090, 27.9986, 27.9987, 28.0052, 27.9845, 27.9867,\n",
      "         27.9922, 27.9780, 27.9815, 28.0107, 28.0084, 28.0025, 27.9889, 27.9852,\n",
      "         27.9956, 28.0105, 27.9894, 28.0175, 27.9803, 28.0166, 28.0026, 28.0067,\n",
      "         27.9880, 28.0034, 27.9935, 28.0057, 27.9883, 28.0112, 28.0121, 28.0063,\n",
      "         28.0119, 28.0083, 28.0086, 28.0088, 27.9983, 27.9960, 27.9925, 28.0116,\n",
      "         28.0031, 27.9898, 28.0102, 28.0070, 27.9880, 27.9940, 27.9908, 28.0260,\n",
      "         28.0073, 28.0053, 28.0079, 27.9988, 27.9931, 28.0078, 28.0090, 27.9957,\n",
      "         27.9984, 27.9920, 28.0181, 28.0112, 27.9931, 28.0116, 27.9976, 28.0011,\n",
      "         27.9963, 27.9944, 27.9983, 27.9840, 28.0031, 27.9992, 28.0041, 28.0063,\n",
      "         28.0039, 27.9893, 28.0020, 27.9951, 28.0047, 28.0032, 27.9967, 27.9907,\n",
      "         28.0014, 28.0005, 28.0066, 28.0210, 28.0055, 27.9965, 28.0010, 27.9876,\n",
      "         27.9967, 28.0025, 28.0057, 28.0032, 28.0195, 27.9932, 28.0038, 28.0117,\n",
      "         27.9899, 28.0199, 28.0113, 28.0078, 27.9909, 28.0073, 27.9981, 28.0242,\n",
      "         28.0037, 27.9986, 28.0098, 27.9830, 28.0002, 27.9992, 27.9893, 28.0089,\n",
      "         27.9935, 28.0039, 28.0247, 27.9909, 27.9982, 28.0106, 27.9928, 28.0093,\n",
      "         28.0194, 28.0200, 27.9675, 28.0176, 28.0022, 28.0022, 27.9977, 28.0005,\n",
      "         28.0008, 19.6451, 14.8222, 22.0774, 13.9411,  9.9979,  6.2688, 23.3892,\n",
      "         28.0222, 28.0138, 27.9823, 27.9990, 27.9819, 28.0239, 28.0013, 28.0073,\n",
      "         28.0142, 28.0007, 27.9978, 27.9979, 28.0110, 28.0095, 28.0160, 27.9918,\n",
      "         28.0063, 28.0140, 27.9908, 27.9962, 19.0129, 21.2107, 27.5631, 27.7697,\n",
      "         26.6664, 18.5544, 27.5590, 17.6775, 23.6170, 27.9960, 27.9900, 27.9948,\n",
      "         27.9975, 27.9931, 28.0070, 28.0094, 27.9764, 27.9817, 28.0101, 28.0103,\n",
      "         28.0051, 28.0108, 27.9959, 27.9968, 28.0076, 28.0049, 28.0171,  2.0933,\n",
      "         16.6014, 27.5570, 21.8435, 17.6814, 27.5761,  3.4131, 18.7821, 27.5500,\n",
      "         17.0064, 26.0156, 28.0098, 27.9929, 27.9948, 28.0100, 28.0049, 28.0043,\n",
      "         28.0098, 28.0006, 27.9975, 27.9923, 28.0006, 28.0054, 28.0137, 27.9986,\n",
      "         27.9868, 27.9983,  1.6791, 26.6916, 27.5566,  8.6804, 18.9966,  5.1746,\n",
      "         11.9862, 23.3935,  5.3849, 26.0337, 27.7906, 14.1786, 28.0072, 28.0043,\n",
      "         28.0054, 27.9975, 27.9984, 28.0027, 28.0031, 28.0086, 28.0214, 27.9876,\n",
      "         28.0016, 28.0176, 27.9834, 28.0191, 27.9996,  6.4857, 27.7841, 27.5617,\n",
      "         17.4637, 20.3307, 28.0126, 27.9902, 27.9954, 27.9755, 26.6873, 10.6685,\n",
      "         13.5020, 24.0459, 15.2600, 28.0164, 27.9860, 28.0060, 28.0028, 28.0190,\n",
      "         28.0016, 28.0036, 28.0055, 28.0072, 27.9969, 27.9912, 27.9960, 28.0075,\n",
      "         18.1101, 27.7700, 28.0117, 27.7955, 20.0892, 27.9870, 27.9925, 28.0094,\n",
      "         28.0050, 27.9908, 27.9981, 28.0003,  4.7299, 27.7781, 12.6143, 28.0074,\n",
      "         28.0016, 28.0204, 28.0061, 28.0243, 27.9934, 28.0139, 27.9955, 27.9919,\n",
      "         27.9741, 27.9792, 28.0099, 23.3831, 21.1862, 27.5663, 27.7733,  0.6168,\n",
      "         27.3573, 28.0030, 28.0247, 28.0162, 28.0148, 27.9962, 27.9949, 28.0033,\n",
      "         26.8941,  5.3821, 21.1856, 25.3529, 28.0096, 28.0063, 28.0233, 27.9871,\n",
      "         27.9958, 28.0030, 28.0072, 28.0109, 28.0086, 28.0099, 28.0114, 19.8619,\n",
      "         27.5618, 27.5699,  6.9318, 26.0227, 28.0061, 28.0101, 28.0107, 28.0015,\n",
      "         27.9922, 27.9910, 27.9999, 27.9992, 28.0129, 13.0554, 26.2494, 17.9025,\n",
      "         27.9960, 27.9973, 27.9967, 28.0069, 28.0010, 28.0270, 28.0073, 27.9882,\n",
      "         28.0028, 28.0027, 28.0072, 11.3047, 27.5675, 27.5590, 27.9987, 28.0139,\n",
      "         27.9918, 28.0006, 28.0037, 28.0273, 28.0090, 28.0010, 27.9929, 28.0047,\n",
      "         28.0002, 14.6104, 22.9376, 22.7302, 28.0047, 28.0131, 28.0045, 28.0026,\n",
      "         27.9846, 27.9969, 27.9896, 28.0116, 27.9785, 28.0083, 27.9987,  6.9088,\n",
      "         27.5679, 27.5621, 27.9887, 28.0155, 28.0016, 27.9902, 28.0238, 28.0037,\n",
      "         28.0133, 28.0074, 28.0038, 28.0044, 28.0054,  3.1843, 24.7072, 20.3069,\n",
      "         27.9954, 28.0071, 28.0055, 27.9992, 27.9774, 27.9902, 27.9890, 27.9911,\n",
      "         28.0083, 28.0121, 28.0147, 12.8555, 27.7947, 22.0803, 27.9959, 27.9842,\n",
      "         27.9802, 27.9974, 27.9938, 27.9975, 27.9955, 28.0025, 27.9894, 27.9980,\n",
      "         28.0054, 15.9188, 25.3678, 19.4278, 28.0010, 28.0033, 28.0074, 27.9898,\n",
      "         27.9979, 28.0130, 27.9831, 27.9850, 28.0038, 28.0013, 16.7893, 26.9054,\n",
      "         27.5630, 10.8735, 28.0009, 27.9996, 27.9984, 28.0122, 28.0055, 28.0116,\n",
      "         28.0210, 28.0147, 27.9864, 27.9965, 18.7790, 23.1629, 19.4380, 28.0022,\n",
      "         28.0242, 28.0066, 27.9842, 28.0085, 27.9870, 27.9982, 27.9903, 27.9925,\n",
      "         28.0173, 28.0082, 15.9076, 27.5434, 27.5441, 12.1891, 28.0075, 28.0150,\n",
      "         27.9928, 28.0068, 27.9923, 27.9899, 27.9925, 27.9900, 27.9928, 28.0029,\n",
      "          1.8889, 27.5540,  9.5726, 27.9987, 27.9930, 28.0020, 28.0051, 27.9767,\n",
      "         28.0099, 27.9906, 28.0190, 28.0046, 28.0009, 27.9845, 20.9764, 22.7280,\n",
      "         27.5707,  9.3355, 27.9955, 28.0056, 28.0013, 28.0060, 28.0079, 27.9846,\n",
      "         27.9805, 28.0191, 27.9933, 19.2181, 23.3961, 27.5468, 15.9293, 27.9988,\n",
      "         28.0043, 28.0017, 27.9819, 28.0058, 27.9995, 28.0134, 27.9927, 28.0111,\n",
      "         27.9904, 28.0066, 27.9931, 14.3909, 27.5673, 27.5592, 27.9905, 28.0164,\n",
      "         28.0023, 27.9974, 28.0027, 28.0045, 27.9951, 28.0001, 19.2112, 11.7531,\n",
      "         27.7638, 11.7488, 25.5787, 28.0106, 28.0076, 27.9940, 27.9991, 27.9993,\n",
      "         28.0022, 27.9974, 27.9949, 27.9833, 28.0145, 27.9916, 27.9774, 22.0726,\n",
      "         18.5541, 27.7988,  9.3265, 26.9106, 28.0116, 27.9991, 28.0126, 27.9942,\n",
      "         27.9981, 28.0144,  8.6730, 27.7949, 23.3908, 25.1405, 28.0019, 28.0055,\n",
      "         27.9899, 28.0031, 28.0098, 28.0088, 28.0065, 28.0138, 28.0051, 28.0047,\n",
      "         27.9936, 28.0078, 27.9963, 27.9967,  4.0792, 27.5704, 27.7873,  5.8338,\n",
      "         26.2671, 27.9753, 28.0056, 28.0130, 16.7955,  6.6967, 26.6897, 23.1607,\n",
      "          3.8447, 27.9948, 28.0136, 27.9919, 28.0019, 28.0189, 28.0155, 28.0045,\n",
      "         27.9970, 27.9881, 28.0007, 28.0055, 27.9973, 28.0025, 28.0039, 28.0180,\n",
      "         23.1696,  2.5446, 27.7786, 27.5500, 20.7693, 11.0838,  0.2999, 19.8974,\n",
      "         24.2663, 27.5660, 22.7197, 17.2439, 28.0028, 27.9995, 28.0083, 28.0124,\n",
      "         28.0070, 28.0114, 27.9921, 28.0071, 28.0010, 28.0034, 27.9949, 28.0054,\n",
      "         27.9881, 28.0063, 28.0084, 27.9855, 28.0170, 23.6067,  3.6452, 22.0675,\n",
      "         26.9078, 27.5656, 27.5784, 27.7774, 26.6933, 13.5112,  9.7761, 27.9993,\n",
      "         28.0028, 28.0129, 27.9997, 28.0031, 28.0119, 27.9929, 28.0065, 27.9791,\n",
      "         27.9905, 28.0196, 28.0031, 28.0011, 27.9995, 27.9969, 27.9980, 28.0097,\n",
      "         28.0106, 28.0176, 27.9961, 28.0091, 14.3860, 12.2090,  9.1157,  4.9375,\n",
      "         15.4763, 27.9861, 27.9978, 28.0147, 28.0187, 28.0164, 27.9946, 28.0046,\n",
      "         28.0137, 28.0028, 27.9957, 28.0174, 27.9959, 28.0040, 27.9862, 27.9976,\n",
      "         28.0094, 27.9913, 28.0087, 27.9826, 27.9957, 27.9975, 27.9853, 27.9988,\n",
      "         28.0168, 27.9944, 28.0021, 27.9977, 28.0140, 28.0063, 27.9922, 27.9907,\n",
      "         28.0021, 27.9883, 27.9886, 27.9825, 28.0176, 28.0010, 28.0039, 28.0110,\n",
      "         28.0196, 27.9880, 27.9949, 28.0105, 28.0274, 28.0064, 27.9908, 28.0095,\n",
      "         28.0030, 28.0032, 28.0059, 28.0039, 28.0004, 27.9777, 28.0010, 28.0028,\n",
      "         28.0022, 27.9980, 27.9977, 27.9905, 28.0035, 27.9881, 28.0021, 28.0118,\n",
      "         28.0073, 27.9950, 27.9936, 28.0124, 27.9932, 28.0098, 28.0143, 27.9960,\n",
      "         28.0145, 28.0020, 27.9923, 27.9928, 28.0024, 27.9892, 28.0034, 27.9912,\n",
      "         28.0026, 28.0135, 27.9911, 28.0078, 28.0002, 28.0193, 28.0002, 28.0106,\n",
      "         27.9963, 28.0000, 27.9934, 27.9955, 27.9904, 27.9976, 27.9965, 27.9831]])\n"
     ]
    }
   ],
   "source": [
    "# test the algorithm\n",
    "alpha_bar = scheduler.alphas_cumprod[0].to(x0.dtype).view(-1, 1, 1, 1)\n",
    "sigma_t = torch.sqrt(1.0 - alpha_bar)\n",
    "\n",
    "res = algorithm1_estimate_singular_values(\n",
    "    score_fn,\n",
    "    x0,\n",
    "    torch.tensor([0], device=device),\n",
    "    sigma_t0=sigma_t,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "e6053a67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14389.6553,  7154.7148,  7093.0542,  6952.4077,  6915.6802,  6885.7031,\n",
       "         6852.6895,  6836.5317,  6805.3467,  6789.9946,  6756.7998,  6724.1191,\n",
       "         6701.0488,  6652.0630,  6649.3677,  6620.7788,  6602.7227,  6589.1670,\n",
       "         6557.3999,  6541.3486,  6509.2705,  6502.8354,  6489.8120,  6471.4761,\n",
       "         6447.1035,  6439.8364,  6420.5723,  6410.4448,  6380.8359,  6374.9331,\n",
       "         6360.7500,  6346.0903,  6342.5347,  6332.9487,  6324.4150,  6307.0435,\n",
       "         6296.8218,  6291.5200,  6274.8906,  6264.4751,  6254.9795,  6242.2422,\n",
       "         6227.0479,  6219.4722,  6212.7920,  6194.3359,  6191.7871,  6187.7759,\n",
       "         6173.4741,  6168.7993,  6147.6626,  6145.3076,  6143.3896,  6132.7036,\n",
       "         6124.7983,  6115.0244,  6111.8354,  6101.3350,  6090.7148,  6085.7070,\n",
       "         6076.7642,  6069.1475,  6061.4912,  6054.0449,  6050.1665,  6047.8818,\n",
       "         6041.3452,  6021.9492,  6010.3721,  6002.1553,  5996.1616,  5993.4155,\n",
       "         5989.8389,  5977.1655,  5972.9390,  5970.6045,  5955.5977,  5951.8335,\n",
       "         5942.5669,  5938.2988,  5932.0298,  5925.2388,  5923.3931,  5913.2744,\n",
       "         5904.8105,  5897.5269,  5893.5513,  5884.6758,  5872.4355,  5862.4224,\n",
       "         5854.0947,  5849.8970,  5848.4028,  5841.1274,  5837.5913,  5826.2744,\n",
       "         5816.7871,  5811.5273,  5807.3555,  5795.1865,  5789.3901,  5781.4282,\n",
       "         5779.6943,  5774.1055,  5770.3740,  5768.6470,  5752.4297,  5747.4014,\n",
       "         5743.2852,  5738.8760,  5729.1504,  5723.0840,  5721.7310,  5713.9253,\n",
       "         5710.4556,  5700.9893,  5699.4087,  5692.6670,  5686.4561,  5680.5103,\n",
       "         5674.6079,  5673.7480,  5670.2046,  5660.3794,  5656.9419,  5648.4385,\n",
       "         5642.7134,  5638.5986,  5632.4580,  5627.5859,  5622.2241,  5618.2373,\n",
       "         5610.6533,  5601.0610,  5598.5718,  5593.6265,  5583.9766,  5581.0337,\n",
       "         5578.0513,  5572.2778,  5567.7046,  5562.4756,  5558.3530,  5555.1133,\n",
       "         5550.0894,  5540.8140,  5533.0020,  5528.1035,  5525.2798,  5520.4722,\n",
       "         5513.8579,  5509.0664,  5502.3345,  5498.7998,  5495.5059,  5489.4771,\n",
       "         5481.4902,  5478.2197,  5471.6626,  5469.0342,  5467.2544,  5462.0293,\n",
       "         5457.5938,  5447.4048,  5442.9629,  5438.2676,  5437.6504,  5423.5625,\n",
       "         5422.2388,  5414.5327,  5407.8325,  5404.0835,  5402.0137,  5395.7773,\n",
       "         5389.1875,  5385.8037,  5377.2314,  5376.2905,  5373.4429,  5367.5127,\n",
       "         5364.3257,  5360.1323,  5356.6562,  5353.3301,  5346.1455,  5333.4595,\n",
       "         5329.4658,  5327.5146,  5322.4727,  5320.5762,  5312.8667,  5309.7598,\n",
       "         5308.3057,  5304.1992,  5299.5366,  5295.4775,  5287.8511,  5285.2402,\n",
       "         5278.1372,  5276.0264,  5273.5415,  5266.3696,  5263.5977,  5260.8052,\n",
       "         5256.8745,  5248.0361,  5245.3193,  5239.1304,  5232.3550,  5229.1230,\n",
       "         5225.7500,  5217.0674,  5215.5986,  5208.3062,  5204.7656,  5199.1274,\n",
       "         5198.7783,  5191.4443,  5187.3833,  5185.3003,  5182.9263,  5178.7407,\n",
       "         5167.7158,  5164.1230,  5159.3892,  5154.3145,  5150.8672,  5149.9990,\n",
       "         5139.8516,  5136.5732,  5132.6816,  5125.7798,  5123.8501,  5112.3555,\n",
       "         5110.3018,  5108.8428,  5104.0630,  5103.0332,  5100.0420,  5095.8530,\n",
       "         5088.0488,  5082.6733,  5078.6763,  5074.1045,  5069.6079,  5064.3813,\n",
       "         5060.5479,  5059.6768,  5053.0176,  5050.1475,  5044.2212,  5042.3882,\n",
       "         5036.6069,  5031.9141,  5030.6582,  5026.2231,  5021.1421,  5012.4932,\n",
       "         5007.1577,  5005.4985,  5000.9683,  4995.4248,  4992.8145,  4987.4370,\n",
       "         4980.5024,  4977.6528,  4975.5703,  4972.9131,  4964.6245,  4961.8740,\n",
       "         4955.5254,  4953.2358,  4948.4917,  4944.0972,  4942.4741,  4934.9077,\n",
       "         4930.1724,  4927.8623,  4921.2593,  4917.4688,  4911.9233,  4905.6040,\n",
       "         4905.1074,  4896.1411,  4894.7695,  4892.5635,  4886.9424,  4882.6475,\n",
       "         4878.3794,  4868.0151,  4864.5420,  4860.6001,  4856.6489,  4854.2114,\n",
       "         4853.5615,  4850.4316,  4847.2036,  4839.3247,  4837.7197,  4832.6094,\n",
       "         4826.1431,  4822.3398,  4820.8237,  4817.9175,  4814.2954,  4809.2446,\n",
       "         4806.2915,  4795.6660,  4795.1646,  4793.4531,  4788.7739,  4784.6372,\n",
       "         4776.2964,  4774.1216,  4770.4775,  4767.2119,  4761.5728,  4757.8281,\n",
       "         4752.2227,  4745.9951,  4744.7734,  4742.8335,  4732.6191,  4729.3311,\n",
       "         4725.2324,  4722.1211,  4718.5024,  4713.4658,  4708.5537,  4705.7593,\n",
       "         4697.8887,  4693.3003,  4691.4912,  4688.9263,  4684.6514,  4677.0601,\n",
       "         4673.8604,  4667.9385,  4660.1484,  4659.0508,  4654.3760,  4649.6201,\n",
       "         4648.2759,  4642.8979,  4641.0596,  4635.3066,  4629.7568,  4626.4458,\n",
       "         4620.6147,  4616.5869,  4613.0962,  4611.5830,  4608.2119,  4604.6367,\n",
       "         4596.8579,  4594.3052,  4592.5073,  4584.6860,  4579.2012,  4575.7197,\n",
       "         4567.2275,  4562.5503,  4558.0498,  4551.9966,  4546.7798,  4542.0820,\n",
       "         4538.4951,  4536.8198,  4534.8579,  4534.1709,  4526.1968,  4521.7700,\n",
       "         4520.9897,  4516.2769,  4510.7007,  4505.2661,  4500.2495,  4495.4585,\n",
       "         4491.4438,  4489.5044,  4479.2944,  4478.6660,  4477.1714,  4474.5015,\n",
       "         4466.7344,  4463.9546,  4458.8052,  4452.8770,  4448.1592,  4444.2046,\n",
       "         4435.3486,  4432.9546,  4428.5342,  4425.0781,  4420.7876,  4418.8301,\n",
       "         4414.0044,  4405.3945,  4401.6851,  4399.0825,  4397.4927,  4391.7559,\n",
       "         4390.7544,  4383.7886,  4381.5264,  4373.2734,  4366.8711,  4363.8828,\n",
       "         4356.4712,  4352.4673,  4347.5527,  4339.8916,  4337.2144,  4329.5190,\n",
       "         4328.2969,  4324.6377,  4317.7578,  4311.2646,  4308.9678,  4306.0928,\n",
       "         4301.2461,  4294.3672,  4292.2666,  4287.7129,  4283.1167,  4282.2998,\n",
       "         4271.8296,  4269.0957,  4264.7988,  4260.9971,  4254.9492,  4250.6016,\n",
       "         4241.3354,  4236.6553,  4232.2285,  4224.4185,  4222.9429,  4218.9380,\n",
       "         4215.6509,  4213.8789,  4203.6592,  4198.1367,  4194.2310,  4188.5679,\n",
       "         4186.0352,  4181.6113,  4177.1465,  4171.0708,  4168.4985,  4164.4448,\n",
       "         4153.2446,  4149.9517,  4145.4619,  4140.9658,  4138.4668,  4134.5225,\n",
       "         4126.6934,  4124.3413,  4121.5874,  4114.8911,  4112.1982,  4105.6997,\n",
       "         4101.6387,  4093.9941,  4093.0549,  4087.7922,  4086.3298,  4075.1001,\n",
       "         4070.7637,  4066.3394,  4059.2319,  4055.2024,  4050.1450,  4037.7317,\n",
       "         4036.6895,  4030.3003,  4026.1565,  4023.5691,  4019.9460,  4014.0017,\n",
       "         4009.3899,  4004.9966,  4002.7012,  3995.3535,  3986.1045,  3980.9116,\n",
       "         3979.1743,  3971.9133,  3970.4194,  3969.2278,  3960.8325,  3953.7593,\n",
       "         3947.1218,  3944.2478,  3934.6826,  3931.1763,  3928.8101,  3919.4675,\n",
       "         3917.7148,  3908.1541,  3899.7510,  3893.0859,  3886.9390,  3883.9590,\n",
       "         3878.5344,  3875.3286,  3873.9043,  3869.5234,  3864.0151,  3851.3279,\n",
       "         3846.4836,  3841.8169,  3833.9146,  3829.5745,  3823.1726,  3816.9419,\n",
       "         3813.6387,  3811.8865,  3802.8425,  3796.3037,  3791.0356,  3786.5930,\n",
       "         3778.2639,  3774.0530,  3769.7288,  3766.6643,  3752.2441,  3739.8599,\n",
       "         3733.9822,  3728.3618,  3722.7227,  3715.9761,  3706.2129,  3697.7275,\n",
       "         3688.4199,  3684.9011,  3676.2058,  3675.0618,  3669.4648,  3659.1333,\n",
       "         3655.2305,  3652.9409,  3643.2246,  3637.4053,  3631.8904,  3626.2415,\n",
       "         3611.8281,  3609.7495,  3603.6191,  3598.2454,  3587.5061,  3580.7219,\n",
       "         3579.5029,  3574.8352,  3564.9612,  3562.5955,  3552.0869,  3544.2783,\n",
       "         3536.7749,  3534.5513,  3526.3176,  3513.1235,  3502.3206,  3498.7185,\n",
       "         3492.8762,  3489.8545,  3478.7405,  3472.3718,  3466.9495,  3462.1987,\n",
       "         3446.8894,  3445.2769,  3437.9053,  3431.8120,  3427.2251,  3421.5750,\n",
       "         3416.6289,  3403.4915,  3399.7278,  3386.3064,  3380.1411,  3370.5649,\n",
       "         3368.9282,  3365.0515,  3355.9143,  3344.6353,  3336.4016,  3332.9011,\n",
       "         3320.9902,  3310.3909,  3304.4421,  3295.4475,  3286.3267,  3281.4724,\n",
       "         3272.1079,  3266.6414,  3254.9810,  3249.3577,  3243.6379,  3238.9060,\n",
       "         3228.0933,  3216.4797,  3212.4827,  3205.7771,  3200.4338,  3189.5251,\n",
       "         3182.6152,  3174.0833,  3160.6150,  3154.6145,  3148.4385,  3137.6316,\n",
       "         3129.0442,  3115.0938,  3105.8345,  3093.8291,  3085.9592,  3078.9358,\n",
       "         3074.7612,  3062.2263,  3058.2520,  3046.4998,  3041.9802,  3033.6570,\n",
       "         3024.9514,  3019.5244,  3012.9116,  2997.3916,  2992.0403,  2981.2361,\n",
       "         2969.4277,  2957.8845,  2952.7112,  2937.1816,  2933.4990,  2920.4751,\n",
       "         2916.7996,  2899.9021,  2892.1448,  2889.0432,  2868.1343,  2863.1277,\n",
       "         2854.4177,  2851.0508,  2825.7627,  2817.9265,  2811.7947,  2805.2646,\n",
       "         2795.8679,  2789.8662,  2780.5320,  2768.9753,  2758.3872,  2747.8667,\n",
       "         2734.6733,  2725.7563,  2716.0413,  2706.9614,  2698.7251,  2687.7214,\n",
       "         2673.0603,  2668.8862,  2656.3838,  2642.3101,  2630.0090,  2598.7188,\n",
       "         2591.8623,  2583.9521,  2583.1094,  2553.8516,  2543.3967,  2531.5986,\n",
       "         2519.2827,  2504.4702,  2497.4456,  2489.6257,  2481.0769,  2454.2083,\n",
       "         2432.2002,  2422.1274,  2417.4441,  2407.8188,  2369.9031,  2363.6111,\n",
       "         2355.9866,  2351.3149,  2337.7192,  2325.0352,  2294.8103,  2278.9619,\n",
       "         2268.9958,  2244.7844,  2234.8252,  2222.8818,  2208.4624,  2183.3137,\n",
       "         2180.3447,  2159.6931,  2149.8525,  2129.5911,  2105.0859,  2091.5713,\n",
       "         2084.5586,  2055.2383,  2031.6088,  2020.4121,  2000.6945,  1982.4115,\n",
       "         1974.3127,  1953.1206,  1917.0885,  1888.6525,  1871.7186,  1852.6404,\n",
       "         1844.6984,  1822.2180,  1820.6056,  1795.2015,  1755.5247,  1731.3326,\n",
       "         1690.9403,  1670.1746,  1662.0267,  1645.1218,  1627.2384,  1604.1636,\n",
       "         1573.3781,  1556.6882,  1541.6073,  1535.2086,  1505.1799,  1447.2208,\n",
       "         1417.3562,  1414.6759,  1392.0806,  1366.0220,  1355.6730,  1345.6255,\n",
       "         1309.4592,  1264.6449,  1222.1715,  1197.0087,  1187.2266,  1143.5486,\n",
       "         1124.4690,  1098.2653,  1059.3873,  1040.8253,  1029.4510,   993.2715,\n",
       "          954.6044,   926.0441,   913.4684,   865.9757,   844.8599,   840.7737,\n",
       "          823.8925,   787.6612,   759.8005,   728.0977,   694.2679,   679.4562,\n",
       "          645.3480,   630.2554,   602.9330,   547.8102,   526.0872,   509.3678,\n",
       "          461.0371,   447.8045,   404.7263,   389.5380,   331.2733,   308.6070,\n",
       "          276.8266,   255.6029,   227.6844,   203.2907,   163.4794,   160.6992,\n",
       "          134.2900,   102.3017,    54.7790,    15.6419])"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sing_val = res[0]\n",
    "sing_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "3bc8425c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.9913818 , 0.9717239 , 0.9665906 , 0.9624008 ,\n",
       "       0.9577865 , 0.9555282 , 0.9511695 , 0.9490238 , 0.9443842 ,\n",
       "       0.93981653, 0.93659204, 0.9297454 , 0.9293687 , 0.92537284,\n",
       "       0.9228492 , 0.9209545 , 0.9165145 , 0.91427106, 0.90978754,\n",
       "       0.90888816, 0.9070679 , 0.9045051 , 0.9010986 , 0.9000829 ,\n",
       "       0.89739037, 0.8959749 , 0.8918365 , 0.8910115 , 0.88902915,\n",
       "       0.8869802 , 0.8864832 , 0.8851434 , 0.88395065, 0.8815227 ,\n",
       "       0.88009405, 0.879353  , 0.87702876, 0.875573  , 0.8742458 ,\n",
       "       0.87246555, 0.87034184, 0.869283  , 0.8683494 , 0.8657698 ,\n",
       "       0.86541355, 0.8648529 , 0.86285394, 0.86220056, 0.8592464 ,\n",
       "       0.8589172 , 0.85864913, 0.85715556, 0.85605067, 0.8546846 ,\n",
       "       0.85423887, 0.8527712 , 0.8512869 , 0.85058695, 0.84933704,\n",
       "       0.84827244, 0.84720236, 0.8461616 , 0.8456195 , 0.8453002 ,\n",
       "       0.8443866 , 0.84167564, 0.84005755, 0.8389091 , 0.83807135,\n",
       "       0.83768755, 0.83718765, 0.8354163 , 0.8348256 , 0.8344993 ,\n",
       "       0.8324018 , 0.8318757 , 0.83058053, 0.829984  , 0.82910776,\n",
       "       0.8281586 , 0.82790065, 0.8264864 , 0.82530344, 0.8242854 ,\n",
       "       0.8237297 , 0.8224892 , 0.8207784 , 0.8193789 , 0.81821495,\n",
       "       0.81762826, 0.8174194 , 0.81640255, 0.8159083 , 0.8143266 ,\n",
       "       0.81300056, 0.8122654 , 0.8116823 , 0.80998147, 0.8091713 ,\n",
       "       0.8080585 , 0.80781615, 0.807035  , 0.8065135 , 0.8062721 ,\n",
       "       0.80400544, 0.80330265, 0.80272734, 0.8021111 , 0.80075175,\n",
       "       0.79990387, 0.79971474, 0.79862374, 0.7981388 , 0.7968157 ,\n",
       "       0.7965948 , 0.7956525 , 0.7947844 , 0.7939534 , 0.79312843,\n",
       "       0.79300827, 0.792513  , 0.7911398 , 0.7906593 , 0.7894708 ,\n",
       "       0.7886706 , 0.78809553, 0.7872372 , 0.7865563 , 0.7858069 ,\n",
       "       0.78524965, 0.78418964, 0.78284895, 0.78250104, 0.78180987,\n",
       "       0.7804611 , 0.7800498 , 0.7796329 , 0.778826  , 0.7781868 ,\n",
       "       0.7774559 , 0.7768797 , 0.7764269 , 0.77572477, 0.77442837,\n",
       "       0.77333647, 0.77265185, 0.77225715, 0.7715852 , 0.77066076,\n",
       "       0.76999104, 0.7690501 , 0.7685561 , 0.76809573, 0.7672531 ,\n",
       "       0.76613677, 0.76567966, 0.7647632 , 0.76439583, 0.76414704,\n",
       "       0.76341677, 0.7627968 , 0.76137275, 0.7607519 , 0.76009566,\n",
       "       0.7600094 , 0.75804037, 0.75785536, 0.75677824, 0.7558418 ,\n",
       "       0.7553178 , 0.7550285 , 0.7541569 , 0.7532358 , 0.75276285,\n",
       "       0.75156474, 0.75143325, 0.7510352 , 0.75020635, 0.7497609 ,\n",
       "       0.74917483, 0.748689  , 0.7482241 , 0.7472199 , 0.7454468 ,\n",
       "       0.74488866, 0.7446159 , 0.7439112 , 0.74364614, 0.7425686 ,\n",
       "       0.7421344 , 0.74193114, 0.7413572 , 0.7407055 , 0.7401382 ,\n",
       "       0.7390722 , 0.7387073 , 0.7377145 , 0.73741955, 0.7370722 ,\n",
       "       0.7360698 , 0.73568237, 0.7352921 , 0.7347427 , 0.7335074 ,\n",
       "       0.73312765, 0.7322626 , 0.7313157 , 0.7308639 , 0.7303925 ,\n",
       "       0.72917897, 0.7289736 , 0.7279544 , 0.72745955, 0.7266715 ,\n",
       "       0.7266227 , 0.7255977 , 0.72503006, 0.7247389 , 0.7244071 ,\n",
       "       0.7238221 , 0.72228116, 0.721779  , 0.7211174 , 0.7204081 ,\n",
       "       0.7199263 , 0.71980494, 0.71838665, 0.7179284 , 0.7173845 ,\n",
       "       0.7164199 , 0.71615016, 0.7145436 , 0.7142565 , 0.7140526 ,\n",
       "       0.71338457, 0.7132406 , 0.71282256, 0.71223706, 0.7111463 ,\n",
       "       0.710395  , 0.7098363 , 0.7091973 , 0.7085688 , 0.7078383 ,\n",
       "       0.7073025 , 0.7071808 , 0.70625   , 0.7058489 , 0.7050206 ,\n",
       "       0.70476437, 0.70395637, 0.7033004 , 0.7031249 , 0.70250505,\n",
       "       0.70179486, 0.700586  , 0.6998403 , 0.6996084 , 0.6989752 ,\n",
       "       0.6982004 , 0.69783556, 0.69708395, 0.6961147 , 0.69571644,\n",
       "       0.6954254 , 0.695054  , 0.6938955 , 0.69351107, 0.69262373,\n",
       "       0.6923037 , 0.6916407 , 0.69102645, 0.6907996 , 0.689742  ,\n",
       "       0.6890802 , 0.68875736, 0.68783444, 0.6873046 , 0.6865296 ,\n",
       "       0.68564636, 0.6855769 , 0.6843237 , 0.68413204, 0.6838237 ,\n",
       "       0.68303806, 0.6824377 , 0.6818412 , 0.6803926 , 0.6799072 ,\n",
       "       0.6793562 , 0.678804  , 0.6784633 , 0.67837244, 0.677935  ,\n",
       "       0.67748386, 0.6763826 , 0.6761583 , 0.675444  , 0.6745402 ,\n",
       "       0.67400867, 0.6737968 , 0.67339057, 0.67288435, 0.6721784 ,\n",
       "       0.6717656 , 0.6702805 , 0.6702104 , 0.6699712 , 0.66931725,\n",
       "       0.668739  , 0.6675733 , 0.6672693 , 0.66675997, 0.6663036 ,\n",
       "       0.66551536, 0.664992  , 0.66420853, 0.6633381 , 0.66316736,\n",
       "       0.6628962 , 0.66146857, 0.661009  , 0.66043615, 0.6600013 ,\n",
       "       0.65949553, 0.65879154, 0.658105  , 0.6577144 , 0.65661436,\n",
       "       0.6559731 , 0.65572023, 0.6553617 , 0.65476424, 0.6537032 ,\n",
       "       0.653256  , 0.6524283 , 0.65133953, 0.6511861 , 0.6505327 ,\n",
       "       0.649868  , 0.6496801 , 0.64892846, 0.6486715 , 0.64786744,\n",
       "       0.64709175, 0.646629  , 0.64581394, 0.645251  , 0.6447631 ,\n",
       "       0.64455163, 0.64408046, 0.64358073, 0.64249355, 0.64213675,\n",
       "       0.64188546, 0.64079225, 0.6400257 , 0.63953906, 0.63835216,\n",
       "       0.6376984 , 0.6370694 , 0.6362233 , 0.6354942 , 0.6348376 ,\n",
       "       0.6343363 , 0.6341021 , 0.6338279 , 0.6337319 , 0.63261735,\n",
       "       0.63199866, 0.6318896 , 0.6312309 , 0.6304515 , 0.6296919 ,\n",
       "       0.62899077, 0.6283211 , 0.62776   , 0.6274889 , 0.6260619 ,\n",
       "       0.62597406, 0.6257652 , 0.625392  , 0.62430644, 0.6239179 ,\n",
       "       0.62319815, 0.6223696 , 0.6217102 , 0.62115747, 0.6199197 ,\n",
       "       0.6195851 , 0.61896724, 0.6184842 , 0.6178845 , 0.61761093,\n",
       "       0.61693645, 0.6157331 , 0.6152146 , 0.6148509 , 0.6146287 ,\n",
       "       0.6138268 , 0.61368686, 0.6127133 , 0.6123971 , 0.61124355,\n",
       "       0.6103487 , 0.60993105, 0.6088952 , 0.60833555, 0.6076486 ,\n",
       "       0.6065779 , 0.6062037 , 0.6051281 , 0.6049573 , 0.6044459 ,\n",
       "       0.6034843 , 0.60257673, 0.6022557 , 0.60185385, 0.60117644,\n",
       "       0.600215  , 0.5999214 , 0.59928495, 0.5986425 , 0.5985284 ,\n",
       "       0.597065  , 0.59668285, 0.59608227, 0.59555095, 0.59470564,\n",
       "       0.594098  , 0.5928029 , 0.5921487 , 0.59153   , 0.5904384 ,\n",
       "       0.5902322 , 0.5896724 , 0.58921295, 0.5889653 , 0.58753693,\n",
       "       0.58676505, 0.5862192 , 0.58542764, 0.58507365, 0.5844554 ,\n",
       "       0.5838313 , 0.5829821 , 0.5826226 , 0.58205605, 0.5804906 ,\n",
       "       0.5800303 , 0.5794028 , 0.5787744 , 0.5784251 , 0.5778738 ,\n",
       "       0.57677954, 0.5764508 , 0.5760659 , 0.57513   , 0.57475364,\n",
       "       0.5738453 , 0.5732777 , 0.57220924, 0.572078  , 0.57134247,\n",
       "       0.571138  , 0.5695685 , 0.5689624 , 0.568344  , 0.5673506 ,\n",
       "       0.5667874 , 0.5660806 , 0.5643456 , 0.5641999 , 0.5633069 ,\n",
       "       0.56272775, 0.5623661 , 0.5618597 , 0.5610289 , 0.5603843 ,\n",
       "       0.5597703 , 0.55944943, 0.55842245, 0.55712974, 0.55640393,\n",
       "       0.5561611 , 0.5551463 , 0.5549375 , 0.55477095, 0.5535975 ,\n",
       "       0.5526089 , 0.5516812 , 0.55127954, 0.5499426 , 0.54945254,\n",
       "       0.5491218 , 0.54781604, 0.54757106, 0.5462348 , 0.5450603 ,\n",
       "       0.5441287 , 0.5432696 , 0.54285306, 0.5420949 , 0.54164684,\n",
       "       0.54144776, 0.54083544, 0.5400656 , 0.5382923 , 0.53761524,\n",
       "       0.536963  , 0.53585845, 0.53525186, 0.5343571 , 0.53348625,\n",
       "       0.53302455, 0.53277963, 0.5315156 , 0.5306017 , 0.5298654 ,\n",
       "       0.5292444 , 0.5280803 , 0.52749175, 0.52688736, 0.52645904,\n",
       "       0.52444357, 0.52271265, 0.5218911 , 0.5211056 , 0.5203174 ,\n",
       "       0.51937443, 0.51800984, 0.5168239 , 0.51552296, 0.51503116,\n",
       "       0.5138158 , 0.51365596, 0.51287365, 0.51142967, 0.51088417,\n",
       "       0.51056415, 0.5092061 , 0.50839275, 0.50762194, 0.5068324 ,\n",
       "       0.5048179 , 0.5045274 , 0.5036706 , 0.50291944, 0.5014185 ,\n",
       "       0.5004702 , 0.5002999 , 0.49964747, 0.4982674 , 0.49793676,\n",
       "       0.49646798, 0.4953766 , 0.49432787, 0.49401706, 0.49286628,\n",
       "       0.49102217, 0.48951223, 0.48900878, 0.48819223, 0.4877699 ,\n",
       "       0.48621652, 0.48532638, 0.4845685 , 0.4839045 , 0.48176476,\n",
       "       0.48153937, 0.48050904, 0.4796574 , 0.4790163 , 0.4782266 ,\n",
       "       0.4775353 , 0.4756991 , 0.47517306, 0.47329718, 0.47243547,\n",
       "       0.47109702, 0.47086826, 0.47032642, 0.46904933, 0.46747288,\n",
       "       0.4663221 , 0.46583283, 0.46416807, 0.46268663, 0.46185517,\n",
       "       0.46059802, 0.45932323, 0.45864475, 0.4573359 , 0.45657185,\n",
       "       0.4549421 , 0.45415613, 0.4533567 , 0.45269534, 0.45118406,\n",
       "       0.44956085, 0.4490022 , 0.44806498, 0.44731817, 0.44579348,\n",
       "       0.44482768, 0.4436352 , 0.44175276, 0.44091406, 0.44005087,\n",
       "       0.4385404 , 0.43734017, 0.43539035, 0.4340962 , 0.43241823,\n",
       "       0.43131828, 0.43033662, 0.42975315, 0.42800117, 0.42744568,\n",
       "       0.4258031 , 0.4251714 , 0.4240081 , 0.42279133, 0.4220328 ,\n",
       "       0.42110854, 0.41893935, 0.4181914 , 0.41668132, 0.4150309 ,\n",
       "       0.41341752, 0.41269445, 0.41052392, 0.4100092 , 0.40818888,\n",
       "       0.40767518, 0.40531343, 0.40422922, 0.40379572, 0.4008733 ,\n",
       "       0.40017354, 0.39895618, 0.3984856 , 0.39495113, 0.39385587,\n",
       "       0.39299884, 0.39208615, 0.3907728 , 0.38993394, 0.38862932,\n",
       "       0.38701406, 0.3855342 , 0.38406375, 0.38221976, 0.38097343,\n",
       "       0.37961558, 0.3783465 , 0.37719533, 0.37565738, 0.37360823,\n",
       "       0.37302482, 0.3712774 , 0.36931032, 0.36759102, 0.36321765,\n",
       "       0.36225936, 0.36115375, 0.36103597, 0.35694665, 0.3554854 ,\n",
       "       0.35383642, 0.35211504, 0.35004473, 0.34906292, 0.34796995,\n",
       "       0.34677508, 0.34301972, 0.3399437 , 0.33853585, 0.33788127,\n",
       "       0.33653596, 0.33123654, 0.33035713, 0.32929146, 0.32863852,\n",
       "       0.32673827, 0.32496545, 0.320741  , 0.31852588, 0.31713295,\n",
       "       0.313749  , 0.31235698, 0.3106877 , 0.3086723 , 0.30515733,\n",
       "       0.30474237, 0.30185592, 0.30048054, 0.29764864, 0.2942236 ,\n",
       "       0.29233468, 0.29135454, 0.28725648, 0.28395385, 0.2823889 ,\n",
       "       0.27963302, 0.27707765, 0.2759457 , 0.2729837 , 0.26794758,\n",
       "       0.26397312, 0.26160634, 0.2589398 , 0.25782976, 0.25468773,\n",
       "       0.25446236, 0.25091168, 0.24536613, 0.24198486, 0.2363393 ,\n",
       "       0.23343691, 0.2322981 , 0.22993535, 0.22743581, 0.2242107 ,\n",
       "       0.21990786, 0.21757516, 0.21546733, 0.214573  , 0.21037595,\n",
       "       0.20227513, 0.198101  , 0.19772638, 0.19456828, 0.19092612,\n",
       "       0.18947966, 0.18807535, 0.18302046, 0.17675686, 0.17082043,\n",
       "       0.16730347, 0.16593625, 0.15983146, 0.15716475, 0.15350232,\n",
       "       0.14806843, 0.14547405, 0.14388429, 0.13882755, 0.13342313,\n",
       "       0.1294313 , 0.12767364, 0.12103567, 0.11808435, 0.11751324,\n",
       "       0.11515379, 0.11008981, 0.10619577, 0.10176473, 0.09703642,\n",
       "       0.09496623, 0.09019898, 0.08808953, 0.08427072, 0.07656632,\n",
       "       0.07353014, 0.07119331, 0.06443822, 0.06258874, 0.05656778,\n",
       "       0.05444493, 0.0463014 , 0.04313337, 0.03869149, 0.03572509,\n",
       "       0.03182299, 0.02841353, 0.02284919, 0.0224606 , 0.01876944,\n",
       "       0.0142985 , 0.00765635, 0.00218624], dtype=float32)"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sing_val = res[0]\n",
    "sing_val = sing_val.cpu().numpy()\n",
    "sing_val = sing_val[1:]\n",
    "sing_val /= sing_val.max()\n",
    "sing_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "0dbd288e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x149e9df10>]"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(list(range(len(sing_val))), sing_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
